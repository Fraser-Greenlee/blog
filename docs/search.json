[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "I’m an ML Engineer at Cohere."
  },
  {
    "objectID": "posts/2020-07-08-An-Avatar-game-with-realistic-physics.html",
    "href": "posts/2020-07-08-An-Avatar-game-with-realistic-physics.html",
    "title": "An-Avatar-game-with-realistic-physics",
    "section": "",
    "text": "After watching Avatar: The Last Airbender I wanted to experience bending the elements just like in the show. Of course actually doing this is impossible but could a game give that feeling?\nIt turns out that you can achieve a compelling water bendning effect using Unity! Here’s the end result."
  },
  {
    "objectID": "posts/2020-07-08-An-Avatar-game-with-realistic-physics.html#so-how-did-i-do-this",
    "href": "posts/2020-07-08-An-Avatar-game-with-realistic-physics.html#so-how-did-i-do-this",
    "title": "An-Avatar-game-with-realistic-physics",
    "section": "So how did I do this?",
    "text": "So how did I do this?\nBefore jumping into 3D lets consider the problem in just 2 dimensions. Corona Labs lets you make 2D cross-platform games and it comes with a realistic particle simulator.\nIt comes with this demo below, click below to try it!\n\n\n\n\n\n\nThis gives some interaction with particles but doesn’t create the smooth flows of water you see in the show.\nIt works by making all particles within the orange box match the box’s velocity. This is done by repeatedly running queryRegion which sets particle velocity in a given region. Notably this allows incrimenting particle velocity as well.\nLets picture the first GIF but with imagined forces on the water drawn as arrows.\n\n\n\nKatara water bending.\n\n\nNow to get a bending effect we can recreate the above GIF it with multiple queryRegion boxes.\nUsing the grid above, when a user taps the screen a ring of cells are updated with each pointing to the cursor. To ensure the cursor is always in the middle of a grid cell the whole grid is shifted as the cursor moves between cells.\nFor cell values I make the magnitude’s greater for further out cells as I found this better holds the particles together. To get long streams of water I gradually decrease old cell values.\nBelow I show the cell values with white arrows (longer for greator velocity incriments).\nTry water bending below.\n\n\n\n\n\n\nThen by making particles with different physical properties you get different materials. Ice is a solid group of particles, green acid mixes colour with the blue water, slime is a sticky group of particles while mud is viscous.\n\n\n\nCheckout the 2D source here."
  },
  {
    "objectID": "posts/2020-07-08-An-Avatar-game-with-realistic-physics.html#d-bending",
    "href": "posts/2020-07-08-An-Avatar-game-with-realistic-physics.html#d-bending",
    "title": "An-Avatar-game-with-realistic-physics",
    "section": "3D bending",
    "text": "3D bending\nNow that our 2D bending is pretty cool we’ve just got to extend it into 3D!\nFirst we can use ObiFluids to simulate particles in 3D. Sadly it doesn’t come with a native queryRegion method but we can just make one by partitioning space into a 3D grid.\nWhen running the code iterates over every particle to find its region & adjusts the velocity in the same way as before.\nThis could clearly make a compelling VR game, unfortunatley I am too invested in an ML project right now to do more work. If you would like any other implementation details feel free to reach out to me on Twitter."
  },
  {
    "objectID": "posts/2023-01-28-I-made-a-BABA-IS-YOU-demake.html",
    "href": "posts/2023-01-28-I-made-a-BABA-IS-YOU-demake.html",
    "title": "I made a BABA IS YOU demake!",
    "section": "",
    "text": "I made a BABA IS YOU demake!\nYou can play it here and view the source code here.\nThe whole thing is made with Pyxel a retro game engine for Python. Now with Web Assembly you can easily export Pyxel games to the browser!\n\n\n\nBABA IS YOU demake screenshot."
  },
  {
    "objectID": "posts/2020-08-25-Transformer-VAE-for-Program-Synthesis.html",
    "href": "posts/2020-08-25-Transformer-VAE-for-Program-Synthesis.html",
    "title": "Transformer-VAE-for-Program-Synthesis",
    "section": "",
    "text": "Since releasing a Transformer Variational Autoencoder (T5-VAE) I have been looking into using it as a tool for reasoning.\nI think learning compressed latent spaces is the only form of abstraction deep learning systems are capable of. I think reasoning using these latent codes will be key for future systems.\n{% include info.html text=“T5-VAE reproduces sequences while compressing them into latent vectors with values between -1 and 1. Continuity between latent vectors is ensured, resulting in a semanticly organised space of sequences with each training sample corresponding to a position.” type=“primary” %}\nProgram synthesis involves creating a program to satisfy some input and output examples. To search efficiently intermediate programs need to be evaluated so only relevent areas are searched. This often involves scoring intermediate programs based on how close the output is to the correct one. The search space is then constrained to programs that increase said score.\nIf the programs were represented by positions in latent space and a deep learning model predicted the output state, we would have a loss (against the target output) to evaluate the program effectiveness and use Bayesian Optimisation to search the space.\nTo do this I’m developing a Python interpreter which uses T5-VAE’s to represent code and state named “Latent Executor”. Its currently in development but I’m excited to share what I’ve learnt so far."
  },
  {
    "objectID": "posts/2020-08-25-Transformer-VAE-for-Program-Synthesis.html#motivation",
    "href": "posts/2020-08-25-Transformer-VAE-for-Program-Synthesis.html#motivation",
    "title": "Transformer-VAE-for-Program-Synthesis",
    "section": "",
    "text": "Since releasing a Transformer Variational Autoencoder (T5-VAE) I have been looking into using it as a tool for reasoning.\nI think learning compressed latent spaces is the only form of abstraction deep learning systems are capable of. I think reasoning using these latent codes will be key for future systems.\n{% include info.html text=“T5-VAE reproduces sequences while compressing them into latent vectors with values between -1 and 1. Continuity between latent vectors is ensured, resulting in a semanticly organised space of sequences with each training sample corresponding to a position.” type=“primary” %}\nProgram synthesis involves creating a program to satisfy some input and output examples. To search efficiently intermediate programs need to be evaluated so only relevent areas are searched. This often involves scoring intermediate programs based on how close the output is to the correct one. The search space is then constrained to programs that increase said score.\nIf the programs were represented by positions in latent space and a deep learning model predicted the output state, we would have a loss (against the target output) to evaluate the program effectiveness and use Bayesian Optimisation to search the space.\nTo do this I’m developing a Python interpreter which uses T5-VAE’s to represent code and state named “Latent Executor”. Its currently in development but I’m excited to share what I’ve learnt so far."
  },
  {
    "objectID": "posts/2020-08-25-Transformer-VAE-for-Program-Synthesis.html#architecture",
    "href": "posts/2020-08-25-Transformer-VAE-for-Program-Synthesis.html#architecture",
    "title": "Transformer-VAE-for-Program-Synthesis",
    "section": "Architecture",
    "text": "Architecture\nThe idea is to learn a semanticly organised map of code and state just by looking at tokens. This means the model should learn to organise programing concepts making the resulting space easier to search!\nTo do This T5-VAEs are trained to represent code and state. Their resulting encodings are concatenated and given to what I’m calling an executor which is a T5-encoder which maps the (state + code) encoding into the resulting state. This is then passed to the state autoencoder again to be decoded into the resulting state.\n!["
  },
  {
    "objectID": "posts/2020-08-25-Transformer-VAE-for-Program-Synthesis.html#initial-tests",
    "href": "posts/2020-08-25-Transformer-VAE-for-Program-Synthesis.html#initial-tests",
    "title": "Transformer-VAE-for-Program-Synthesis",
    "section": "Initial Tests",
    "text": "Initial Tests\nThe first test for this model was to learn to find the missing value in simple sums. These were in the form 12+?=23 or 10+?=5.\nHere “state” represents the first value in the sum and the result (both being 2-digit positive integers) while “code” is the summed value (a 2-digit integer).\nA small Latent Executor was made with T5-base encoders and decoders and a just 2 dimensional latent space. By only using 2 latent dimensions the space of “programs” was easy to visualise.\nHere are the resulting state and code latent spaces.\n![\nEven though the model just sees abstract tokens it has still grouped numbers with similar values. The code space looks better organised than the state space. This is caused by a bug resulting from passing the executor’s “Encoder Hidden” output back into the state autoencoder. T5-VAE is an MMD-VAE meaning it computes its regularisation loss using the maximal mean divergence between its latent codes in a given batch and a sample of latent codes from a normal distribution. I forgot to combine the latent codes produced by the executor and those produced by the autoencoder leading to 2 seperate distributions of latent codes trying to ocupy the same space, hence the hole when sampling from state strings.\nA varient with a 10-dimensional latent space was also trained and by using t-SNE with multiple perplexities we can see a similar orginisation of state and code. Lower perplexities show local structure while high perplexities show global structure.\n![\n![\nSince a continuous space of well organised code was found, it was time to try measuring the loss for different “programs” and check its accuracy. The loss used was state-decoder cross entropy which gave the most reliable signal.\n![\nLooking at the loss, the lowest values (coloured white) are at or near -53. You can also see that the space is not perfectly organised, -50 and -51 are at oposite ends of the y-axis. If we were yo use gradient based optimisation (akin to rolling a ball down a hill) it would get stuck in one of the blue patches and likely not find the white patch at -53. This means gradient-based optimisation won’t be effective.\nAnother optimisation method seen in papers such as SD-VAE is bayesian optimisation with Gaussian Processes. Using it reliably finds solutions using latent codes with 2, 10 and 100 dimensions."
  },
  {
    "objectID": "posts/2020-08-25-Transformer-VAE-for-Program-Synthesis.html#next-steps",
    "href": "posts/2020-08-25-Transformer-VAE-for-Program-Synthesis.html#next-steps",
    "title": "Transformer-VAE-for-Program-Synthesis",
    "section": "Next Steps",
    "text": "Next Steps\nAlthough I’m excited to see it working here I want to extend this to handle multiple lines of code. This means using the executors output with another code encoding to send to the executor and produce a new state. That combined with a greater range of values should make a more interesting program synthesiser."
  },
  {
    "objectID": "posts/2020-06-25-a-dataset-of-ran-code.html",
    "href": "posts/2020-06-25-a-dataset-of-ran-code.html",
    "title": "A dataset of ran code",
    "section": "",
    "text": "Existing ML for code methods often learn from raw source code and sometimes use data flow to understand programs. While these offer large amounts of data they don’t actually show what code fundamentally does, changes state.\nWith that in mind I’ve created a dataset of over 1 million ran python programming solutions with state changes shown after each line of code. Get it here"
  },
  {
    "objectID": "posts/2020-06-25-a-dataset-of-ran-code.html#motivation",
    "href": "posts/2020-06-25-a-dataset-of-ran-code.html#motivation",
    "title": "A dataset of ran code",
    "section": "Motivation",
    "text": "Motivation\nMost AI for code models only learn from reading code. This lets AI models recognise patterns in the code & generate sensible-looking code.\nBut do these models really understand code? If you were to read code without running anything and had no prior knowledge of programming would you know how to use it?\n\nAn example\nLets try learning about a single program character the way an AI would.\nBelow is a simple program with just 1 character replaced by a Chinese character making the code inscrutable. This is a similar experience for an AI seeing the true character for the first time.\nAs you can see the code is inscrutable.\nC = A六B\nD = B六C\nE = A六D\nD = C六E\nHow could I make this understandable without showing the true character?\nSince all code does is change state, lets show state changes between each line.\nBelow shows state changes but the values are also represented by Chinese characters.\n.. A = 书\n.. B = 書\nC = A六B\n.. C = 書\nD = B六C\n.. D = 书\nE = A六D\n.. E = 书\nD = C六E\n.. D = 書\nLets look at all the operations with the values substituted in:\n书六書=書, 書六書=书, 书六书=书, 書六书=書\nIf you are familiar with programming you may be able to guess that 六 is an and operator with 书 being True & 書 being False.\nHere’s the original code with swapped words:\n#  and is 六\n#  True is 书\n#  False is 書\nC = A and B\nA = A and C\nD = C and B\nD = D and D\nThink how much more code you would need to read to figure out what 六 meant if you couldn’t see state changes. For this reason I rekon showing state changes will help AI models understand code."
  },
  {
    "objectID": "posts/2020-06-25-a-dataset-of-ran-code.html#how-i-got-this-data",
    "href": "posts/2020-06-25-a-dataset-of-ran-code.html#how-i-got-this-data",
    "title": "A dataset of ran code",
    "section": "How I got this data",
    "text": "How I got this data\nI first found the CodeChef dataset it’s got 1 million solutions to programming problems but few are in Python and many aren’t runnable.\nHackerRank is a site where programmers can practice for interviews on their huge range of programming problems. It has a “See Solutions” button that lets you read other peoples solutions. I used Selenium to click on that button for every problem to get solution URLs. Then I used Requests to download those solutions. After a few days I had 1 million solution code snippets with on average 1k solutions per problem.\n\nRecording State Changes\nOnce I had my runnable code snippets I used Snoop to record all the state changes occurring in each program run.\nSnoop dynamically adds logs to your code showing what has executed & which state values have changed. To see how this works check out this talk.\n\n\n\nSome Snoop output.\n\n\nNext step was to run all the code snippets. The script used to run the code and progress logs can be found here (EDIT: I have unfortunatelty lost the script but you may enjoy looking at some lovely progress graphs instead). Thankfully running all these code snippets didn’t cause many issues, I just had to watch out for some the occasional massive code snippet.\nAn interesting yet worrying feature of the Snoop files is that they are highly compressible, the zip file is 1/10th the size of the txt file. This is likely due to repetitions in the snoop due to the same code being ran repeatedly in for loops and method calls."
  },
  {
    "objectID": "posts/2020-06-25-a-dataset-of-ran-code.html#initial-results",
    "href": "posts/2020-06-25-a-dataset-of-ran-code.html#initial-results",
    "title": "A dataset of ran code",
    "section": "Initial Results",
    "text": "Initial Results\nWith the dataset completed I trained gpt2 to perform causal language modelling on the raw text. I wanted to see if the Snoops text offers the model any unique insights about code.\n\nPre-Training\nHere you can see some initial results on training gpt2 & BERT on the data.\n\n\n\nEval loss while training gpt2-medium from-scratch/pre-trained on the Snoop text.\n\n\nSurprisingly gpt2 with OpenAI pre-training (blue) is actually already very accurate on the Snoop text.\nWhen having it generate text with short prompts I found it to be far less accurate than the 1.1 evaluation perplexity. It seems like the gpt2 model is spotting patterns in it’s prefix string & is using them to predict future tokens rather than having a probabilistic model of an interpreter.\n\n\nA Downstream Task\nHere you can see it fine-tuned on CoNaLa (a description-to-code translation task). The Snoop data is better than no pre-training but not as good as gpt2 with OpenAI pre-training. Of course the Snoops model hasn’t had the same amount of training time nor has it seen natural language before so its not a fair test.\n\n\n\nFine-tuning gpt2 from scratch vs pre-trained.\n\n\n\n\n\nFine-tuning Snoop pre-trained vs OpenAI pre-trained."
  },
  {
    "objectID": "posts/2020-06-25-a-dataset-of-ran-code.html#future-work",
    "href": "posts/2020-06-25-a-dataset-of-ran-code.html#future-work",
    "title": "A dataset of ran code",
    "section": "Future Work",
    "text": "Future Work\nThere’s a lot of potential in using state changes to understand code. Right now I don’t think just reading the raw snoops text is the way to go but I think a well formatted, compressed version could do better. Next time I’ll show how well a compressed version of this dataset can teach a transformer to act as a Python interpreter.\nHere’s some other investigations you could do with this dataset:\n\nFurther testing on how this helps downstream tasks, try training a language model on just the code & just the Snoop, see which does better on downstream tasks & by how much. These downstream tasks could include bug detection, data type prediction, search & description2code generation.\nRun Snoop on the Django source code with the English2Django dataset comments in the code. That way a language model will learn NLP, code & state all at the same time. Note that I got a lot of STATE_UNAVAILABLE messages when I tried this so be ready to filter a lot of data.\nTry applying this method to another programming language, maybe there’s a similar tool for Java or Javascript?\nTry filtering & rearranging the data for new tasks. It offers a valuable insight as to what parts of the source code are useful to humans."
  },
  {
    "objectID": "posts/2021-02-23-An-Improved-Transformer-VAE.html",
    "href": "posts/2021-02-23-An-Improved-Transformer-VAE.html",
    "title": "An-Improved-Transformer-VAE",
    "section": "",
    "text": "I have made an improved Transformer-VAE that gives much more compelling interpolations on a wider range of tasks than my previous work T5-VAE. You can try out training in Colab or check out the source code.\nIn this post I’ll describe the changes I made and what this has taught me about independent ML research."
  },
  {
    "objectID": "posts/2021-02-23-An-Improved-Transformer-VAE.html#motivation",
    "href": "posts/2021-02-23-An-Improved-Transformer-VAE.html#motivation",
    "title": "An-Improved-Transformer-VAE",
    "section": "Motivation",
    "text": "Motivation\nAs outlined in a previous post I think large transformer VAEs have a lot of potential.\nThey let you interpolate between pieces of data (text, images, etc) to find new and plausible combinations. By interpolating between data points we can make machines creative.\nTransformers are the most general type of neural network, able to get top results in images & text with few priors on the data. This means a large scale Transformer-VAE will be able to create better interpolations than any other architecture.\nWith this in mind I set out to improve on my initial Transformer VAE."
  },
  {
    "objectID": "posts/2021-02-23-An-Improved-Transformer-VAE.html#baselines",
    "href": "posts/2021-02-23-An-Improved-Transformer-VAE.html#baselines",
    "title": "An-Improved-Transformer-VAE",
    "section": "Baselines",
    "text": "Baselines\nTo test performance I opted to judge the model on interpolation quality and how semantically organised the latent codes are.\nTo check the semantic organisation of the latent codes I used a news headlines dataset and trained an SVM on the latent codes to predict the news headline category.\nFor interpolation quality I looked at syntactic & semantic correctness. Here I define syntax as a strict set of rules that all samples must follow (like those of a compiler), for semantics I mean that samples qualitatively appear to be a mix between one sample and the other.\nThe syntax test used this python lines dataset and interpolations were tested for syntax errors. For semantics I trained the model to recreate MNIST characters using this dataset and looked to see if the character mixes were credible."
  },
  {
    "objectID": "posts/2021-02-23-An-Improved-Transformer-VAE.html#improvements",
    "href": "posts/2021-02-23-An-Improved-Transformer-VAE.html#improvements",
    "title": "An-Improved-Transformer-VAE",
    "section": "Improvements",
    "text": "Improvements\nFirstly I’ve swapped out the T5-encoder for the Funnel transformer encoder (though still using the shared T5 embeddings). The Funnel transformer is trained to compress sequence data so it doesn’t have to use as many parameters to produce encodings.\nThen I treat each funnel token as its own seperate latent code, this gives me more latent codes to use in the MMD loss. This gives me more tokens to regularise which is important when the total batch size is low.\nWhen creating the reconstructed encoding I use LayerNorm on the reconstructed tokens to match the Funnel encoder.\nFor handling large sequences I added gradient checkpointing and a basic window attention mechanism.\nGradient checkpointing simply ignores the gradients for most layers and recalculates then when backpropagating. This can greatly help save memory and approximates the reversible layers of the Reformer which doesn’t have cross-attention.\nWindow attention only handles operates on subsequences of the data of length widow size. Here I just feed each decoder layer subsequences of the data that overlap between layers. In the future I could replace the self-attention layers with a Longformer self-attention followed by T5 cross attention on subsequences."
  },
  {
    "objectID": "posts/2021-02-23-An-Improved-Transformer-VAE.html#changes-that-didnt-work-out.",
    "href": "posts/2021-02-23-An-Improved-Transformer-VAE.html#changes-that-didnt-work-out.",
    "title": "An-Improved-Transformer-VAE",
    "section": "Changes that didn’t work out.",
    "text": "Changes that didn’t work out.\nThe key idea with Transformer-VAE is that by using a large transformer we can get a consistently valid output regardless of the latent code. Currently interpolation performance doesn’t clearly improve as the model gets more accurate. I measure this by learning lines of Python code and measuring how often interpolations have syntax errors.\nHere are some samples from my best traininig run (note that auto-encoding accuracy is 89% all logs):\n# ratio         sequence                                                valid\n\n0             start_timeperiod = prev_timeperiod                        T\n0.1           start_timeperiod = prev_timeperiod                        T\n0.2           start_timeperiod = prev_timeperiod                        T\n0.3           start_timeperiod = prev_timeperiod                        T\n0.4           start_timeperios = prev_Infood                            T\n0.5           return_qualos = min(vlabo)                                T\n0.6           return summary_stats = min(balance))                      False\n0.7           return summary_stats(balance))                            False\n0.8           return summary_stats(balance))                            False\n0.9           return summary_stats(balance))                            False\n1             return summary_stats(balance))                            False\n\n0             raise ValueError('Infinite observation encountered.')     T\n0.1           raise ValueError('Infinite observation encountered.')     T\n0.2           raise ValueError('Infinite observation encountered.')     T\n0.3           raise ValueError('Infinite observation encountered.')     T\n0.4           raise outError(' prefinite observation_type'.')           False\n0.5           raise out('lert_typeREN_type'.)                           False\n0.6           global outdir_type_type._type                             False\n0.7           global outdir_type_type                                   T\n0.8           global outdir_type_type                                   T\n0.9           global outdir_type_type                                   T\n1             global outdir_type_                                       T\nOverall ~65% of interpolation points were valid. Note that I did not use a Python specific tokenizer which means that some tokens will make syntax errors more likely. One potential way to improve this is to optimize the interpolations directly.\nI tried 3 methods of doing this, none substantially changed performance.\nCritic loss had another funnel encoder operate on the final decoder hidden state to predict the interpolation ratio (inspired by the adviserial interpolations paper). The critic was accurate but it didn’t improve the model.\nCycle loss put a cosine embedding loss on latent VS Encoder(Decoder( latent )). This is to encourage the latent space to become a bijective mapping.\nLastly I tried adding a smoothness loss where the gradient of the logits w.r.t the interpolation ratio was minimized.\nBoth of the above methods were inspired by this paper.\nUnfortunately I didn’t learn a great deal from these methods, they just didn’t update the model that much or lowered performance. This is likely because these methods were original applied to image models where the data is less discrete. Current SOTA for training text transformers with an adversary is by using reinforcement learning which I wanted to stay away from as it would necessitate longer training times."
  },
  {
    "objectID": "posts/2021-02-23-An-Improved-Transformer-VAE.html#conculsion",
    "href": "posts/2021-02-23-An-Improved-Transformer-VAE.html#conculsion",
    "title": "An-Improved-Transformer-VAE",
    "section": "Conculsion",
    "text": "Conculsion\nOverall this project took wayyy longer than I expected. In the future I’m going to try to work more incrementally, making small, test-able experiments at a time.\nTips for your own research side project: - Remember that choosing the right thing to work on is more important than running experiments and writing code. - Start by setting up a small test of your hypothesis, this should be a baseline with a performance metric. - Stay small, reuse open-source code & data where possible and do fast runs on Google Colab. - Lean on the side of caution when trying out a new method. Read related papers where possible so you can skip on less promising ideas.\nIf you want to see even more results you can check out this Weights and Biasis report. I hope to release some cool demos using this project soon! If you want to help out feel free to reach out to @FraserGreenlee.\nThere is a ton of ideas yet to be explored using this project! * What would an ArtBreeder of sequences be like? Could it create compelling writing prompts via interpolation? * Can semantic directions in latent space be found so you can edit texts at a high level? * Does part of the latent codes encode style? If so can that style be applied to other sequences? * The model has a prior on the distribution of latent codes, could that be replaced by a more general loss? * I was keen on smooth latent codes as I hoped to take advantage of their differentiability. Now discrete VQ-VAEs are shown to achieve great results why not look into converting this project into one?"
  },
  {
    "objectID": "posts/2021-06-13-TVAE-JAX.html",
    "href": "posts/2021-06-13-TVAE-JAX.html",
    "title": "Making a Transformer-VAE with JAX.",
    "section": "",
    "text": "JAX allows writing simple code that runs efficiently on TPUs. These models can then operate on massive scales setting new benchmarks in performance.\nFor an intro to JAX & Flax please checkout day 1 talks from the Huggingface Jax sprint.\nhttps://youtu.be/fuAyUQcVzTY\nYou can find the code from this post at t5-vae-python, t5-vae-wiki and the model code t5-vae-flax."
  },
  {
    "objectID": "posts/2021-06-13-TVAE-JAX.html#making-your-own-transformer",
    "href": "posts/2021-06-13-TVAE-JAX.html#making-your-own-transformer",
    "title": "Making a Transformer-VAE with JAX.",
    "section": "Making your own transformer",
    "text": "Making your own transformer\nIts important to remember that deep neural nets learning the same task learn universal features see circuits. This means all transformer architectures are largely equivilent with only small changes mattering.\nWith that in mind you should probably build your new model on existing Transformers.\nFor the Transformer-VAE I wanted to modify a T5 model to average pool the hidden states into one token.\n{% include video.html url=“/videos/avg-pool-t5.mp4” %}\nAnd to autoencode that hidden token with a VAE.\n\n\n\n.\n\n\nWith a regularising MMD-VAE loss that operates on each batch.\n\n\n\n.\n\n\nThis would allow interpolating on sequences."
  },
  {
    "objectID": "posts/2021-06-13-TVAE-JAX.html#how-i-made-the-transformer",
    "href": "posts/2021-06-13-TVAE-JAX.html#how-i-made-the-transformer",
    "title": "Making a Transformer-VAE with JAX.",
    "section": "How I made the Transformer",
    "text": "How I made the Transformer\nThe trained models will have their own git repos on the model hub with the model code as a git submodule shared between all trained models.\nTo make a new Flax Tranformer there’s some boilerplate you’ll need to start off with.\nconfig.py\nAdds the extra configuration options your new transformer will need:\nfrom transformers.configuration_utils import PretrainedConfig\nfrom transformers import AutoConfig\n\nclass T5VaeConfig(PretrainedConfig):\n    model_type = \"transformer_vae\"\n    is_composition = True\n\n    def __init__(\n        self,\n        # custom VAE config\n        t5_model_name_or_path=None,\n        n_latent_tokens=6,\n        latent_token_size=32,\n        ...\n        # T5 config\n        t5=dict(),\n        vocab_size=32128,\n        d_model=512,\n        ...\n        # end\n        **kwargs,\n    ):\n        ...\n        # You can load other configs as attributes to your own config\n        self.t5 = AutoConfig.from_pretrained(t5_model_name_or_path, cache_dir=cache_dir)\n        assertEqual(self.t5.model_type, \"t5\", \"Need t5 model type for transformer_decoder.\")\n        self.t5.decoder_start_token_id = decoder_start_token_id\nt5_vae.py\nThis holds the main source code for the model.\nclass FlaxT5VaeForAutoencodingModule(nn.Module):\n    '''\n        Runs the model inference.\n    '''\n    config: T5VaeConfig\n    dtype: jnp.dtype = jnp.float32  # the dtype of the computation\n\n    def setup(self):\n        # Here we initialise the transformer we're building on\n        self.t5 = FlaxT5ForConditionalGenerationModule(self.config.t5)\n        # And load the VAE to be added.\n        self.vae = VAE(self.config)\n\nclass FlaxT5VaePreTrainedModel(FlaxPreTrainedModel):\n    '''\n        An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained models.\n    '''\n    config_class = T5VaeConfig\n    base_model_prefix = \"transformer\"\n    module_class: nn.Module = None\n\n    def __init__(\n        self,\n        config: T5VaeConfig,\n        input_shape: Tuple[int] = (1, 1),\n        seed: int = 0,\n        dtype: jnp.dtype = jnp.float32,\n        **kwargs\n    ):\n        module = self.module_class(config=config, dtype=dtype, **kwargs)\n        super().__init__(config, module, input_shape=input_shape, seed=seed, dtype=dtype)\n\n\nclass FlaxT5VaeForAutoencoding(FlaxT5VaePreTrainedModel):\n    '''\n        Holds the module class & prepared inputs for inference.\n    '''\n    module_class = FlaxT5VaeForAutoencodingModule\nFor the custom Flax model (in this case a VAE) its just a regular Flax model.\nvae.py\nclass VAE(nn.Module):\n    config: T5VaeConfig\n    dtype: jnp.dtype = jnp.float32  # the dtype of the computation\n\n    def setup(self):\n        self.encoder = VAE_ENCODER_MODELS[self.config.vae_encoder_model](self.config.latent_token_size, self.config.n_latent_tokens)\n        self.decoder = VAE_DECODER_MODELS[self.config.vae_decoder_model](self.config.t5.d_model,  self.config.n_latent_tokens)\n\n    def __call__(self, encoding=None, latent_codes=None):\n        latent_codes = self.encode(encoding)\n        return self.decode(latent_codes), latent_codes\nFor more Flax examples see What does Flax look like."
  },
  {
    "objectID": "posts/2021-06-13-TVAE-JAX.html#training",
    "href": "posts/2021-06-13-TVAE-JAX.html#training",
    "title": "Making a Transformer-VAE with JAX.",
    "section": "Training",
    "text": "Training\nNow that you’ve got your model code setup you can start training!\nFirst make a new huggingface model.\nThis will hold the training code and model weights.\nFor a train script build on one of the Huggingface examples.\nYou just need to switch out the model initialisation."
  },
  {
    "objectID": "posts/2021-06-13-TVAE-JAX.html#debugging",
    "href": "posts/2021-06-13-TVAE-JAX.html#debugging",
    "title": "Making a Transformer-VAE with JAX.",
    "section": "Debugging",
    "text": "Debugging\nJAX is efficient because it gets compiled into XLA. This is great for performance but it makes debugging tricky.\nTo get incremental runs of your code in Python you should build off the tests already in transformers.\nSee test_modeling_flax_t5.py."
  },
  {
    "objectID": "posts/2021-06-13-TVAE-JAX.html#run-training",
    "href": "posts/2021-06-13-TVAE-JAX.html#run-training",
    "title": "Making a Transformer-VAE with JAX.",
    "section": "Run Training",
    "text": "Run Training\nDuring the HF sprint I got to use a TPUv3-8 which allowed training with an 800 batch size with N tokens per sentence.\nSadly this training only started 6 hours before the contest ended so it only saw 5% of wikipedia."
  },
  {
    "objectID": "posts/2021-06-13-TVAE-JAX.html#share-your-work",
    "href": "posts/2021-06-13-TVAE-JAX.html#share-your-work",
    "title": "Making a Transformer-VAE with JAX.",
    "section": "Share your work",
    "text": "Share your work\nFinally you can share your new model using a Streamlit demo on Huggingface spaces.\nCheckout the Transformer-VAE one here."
  },
  {
    "objectID": "posts/2021-06-13-TVAE-JAX.html#final-thoughts",
    "href": "posts/2021-06-13-TVAE-JAX.html#final-thoughts",
    "title": "Making a Transformer-VAE with JAX.",
    "section": "Final thoughts",
    "text": "Final thoughts\nWhile the I wasn’t able to get the performance I wanted with the Transformer-VAE I am excited to see people do with TPUs."
  },
  {
    "objectID": "posts/2025-04-17-A-Pyxel-Playground-Game-Editor.html",
    "href": "posts/2025-04-17-A-Pyxel-Playground-Game-Editor.html",
    "title": "Pyxel Playground: A Web-Based Game Editor",
    "section": "",
    "text": "Pyxel Playground: A Web-Based Game Editor\nI’ve created a browser-based editor for making retro games with Pyxel, a Python retro game engine. It lets you code and play simple games right in your browser.\nTry it here: Pyxel Playground"
  },
  {
    "objectID": "posts/2021-03-11-Interpolating-the-internet.html",
    "href": "posts/2021-03-11-Interpolating-the-internet.html",
    "title": "Interpolating the internet",
    "section": "",
    "text": "Large scale Deep Learning models can combine abstract concepts in new ways. GPT-3 can write stories while DALL·E makes images. These models are improving fast so I want to explore what improved versions will do and how they will change the internet."
  },
  {
    "objectID": "posts/2021-03-11-Interpolating-the-internet.html#the-best-content-generation-today",
    "href": "posts/2021-03-11-Interpolating-the-internet.html#the-best-content-generation-today",
    "title": "Interpolating the internet",
    "section": "The best content generation today",
    "text": "The best content generation today\nThe best AI content generators use large models trained on huge datasets.\nThese models work by storing all their training data in a “latent space”1. Think of this as a map where content is arranged by its properties (e.g. going left words get angrier, pictures become more cat like as you move up, etc). To create new content we can simply adjust our positions to get new combinations of properties.\n\n\n\n.\n\n\nUnlike Google Search which stores links and searches for results, a deep learning model combines aspects of existing content to make new results. A great example of this is DALL·E which works like Google image search on steroids, interpolating between relevent results rather than indexing them.\nBoth are results for the prompt “a road sign with an image of a blue strawberry”:\n![\n![\nNotice that Google’s results are not relevent. It is perfectly plausible for their to be “a road sign with an image of a blue strawberry” but there aren’t many on the internet.\nMeanwhile every one of DALL·E’s generations are relevent. Of course producing these outputs currently takes a long time and is expensive but this tech is scaling fast.\nA similar phenomenon is playing out with text.\nGPT-3 is trained to complete text prompts. Here ideas and concepts can be merged to reason & write stories like in this hilarious example:\n\nAlready people are generating articles using GPT-3 by giving manual prompts.\nIt is important to keep in mind that GPT-3 and DALL·E are almost the same model. They both use a large transformer with DALL·E using a VQ-VAE to compress images into token-like sequences. You can think of this as a learned compiler for images with GPT-3 acting as the programmer.\nThis means we can apply the DALL·E format {content search query} {VQ-VAE tokens} to any form of data. Expect VQ-VAEs to allow a future GPT model to generate any form of content letting it generate full rendered web pages rather then just text."
  },
  {
    "objectID": "posts/2021-03-11-Interpolating-the-internet.html#how-will-these-models-improve",
    "href": "posts/2021-03-11-Interpolating-the-internet.html#how-will-these-models-improve",
    "title": "Interpolating the internet",
    "section": "How will these models improve?",
    "text": "How will these models improve?\nThe large datasets training these models are the same ones used by search engines.\nWhen we search today we hope someone has made something just like what we’re looking fo. With a large scale deep learning model that won’t be the case.\nInstead a search could query a large model thats interpolates between the best results to find that image, article, video, etc that exactly matches your query.\nOnce these “Generators” become mainstream we’re going to have to change our perspective on what the internet is.\nWe will no longer think of something being on or off the internet. The idea that someone has their “nudes posted online” will be a meaningless phrase as you will inevitably generate nudes as you search for them.\nThe only value in “making” images will be in finding the right one, to search latent space to best fulfil someones needs.\nMost content will be generated on demand to a users needs but some will be automated, expect Spotify & YouTube to take full advantage of their huge datasets to create content.\nHaving these generators will feel imensly powerful, finaly I’ll get to make a water bending VR game, the amount of content posted online will tenfold!\nEventually a new internet will emerge. One where rather than searching for a peice of content hand-made by a person, all content is generated to suit our individual needs. This will feel like we each have our own internet. One where all content matches just what we want."
  },
  {
    "objectID": "posts/2021-03-11-Interpolating-the-internet.html#what-you-can-do-now.",
    "href": "posts/2021-03-11-Interpolating-the-internet.html#what-you-can-do-now.",
    "title": "Interpolating the internet",
    "section": "What you can do now.",
    "text": "What you can do now.\nIf we are on the cusp of a new Google how can you take advantage of this? The best thing to do is get involved on early AI content generation tools. Some of these are completely autonomous and some require a human in the loop. Here are some ideas I’ve been considering.\n\nTransformer-VAEs as a search engine\nTo make a new Google your probably going to need a Transformer-VAE.\nI’ve just released a project that allows using the transformer to interpolate over text and small images. Hopefully once I try training at scale I’ll be able to interpolate over entire documents.\nOnce it can interpolate over entire documents why not use it with a search engine to interpolate between the top results to best match a query?\n\n\nAuto ArtBreeder\nArtBreeder uses a mix of VAE’s & GAN’s to generate images by combining latent variables of existing images to interpolate between them. Images are interpolated several times over to generate novel and strickingly different images.\nThis is different to most content generators in that you can’t see an output image and easily know how to find it in latent space. In the future algorithms will be needed to discover new images in these spaces (see Kenith Stanley’s interview for more info). Perhaps processing interpolated images with OpenAI’s new CLIP model will allow discovering new images?\nThe final result would allow for fully automanus brainstorming. Use this to prompt a larger model and you could get some compelling & creative outputs.\n\n\nOther Ideas\nThese ideas work great for very flexible mediums like images and text where slight mstakes can go unnoticed.\nCould some clever fixes allow applying them to strict domains like program synthesis? Maybe you could use latent variables to add/remove concepts in a Python function?"
  },
  {
    "objectID": "posts/2021-03-11-Interpolating-the-internet.html#footnotes",
    "href": "posts/2021-03-11-Interpolating-the-internet.html#footnotes",
    "title": "Interpolating the internet",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nSince these models are trained with gradient descent they approximate non-parametric models (a.k.a. models that reason from their dataset rather than learning programs) see more here.↩︎"
  },
  {
    "objectID": "posts/2020-08-13-Transformers-as-Variational-Autoencoders.html",
    "href": "posts/2020-08-13-Transformers-as-Variational-Autoencoders.html",
    "title": "Transformers-as-Variational-Autoencoders",
    "section": "",
    "text": "Image generators based on Variational Autoencoders have had huge success. Unfortunately the same cannot be said for sequence generation. There seems to be little interest in this space but with Transformers it is now possible to learn smooth latent spaces of large structured sequences.\nTo show this I’m releasing T5-VAE a mod of Google’s Text-to-Text Transformer to learn smooth latent spaces of sequences.\nTry using it in Google Colab.\nSee Weights & Biasis training runs.\nCheck out the source code on GitHub."
  },
  {
    "objectID": "posts/2020-08-13-Transformers-as-Variational-Autoencoders.html#from-autoencoders-to-mmd-vae",
    "href": "posts/2020-08-13-Transformers-as-Variational-Autoencoders.html#from-autoencoders-to-mmd-vae",
    "title": "Transformers-as-Variational-Autoencoders",
    "section": "From Autoencoders to MMD-VAE",
    "text": "From Autoencoders to MMD-VAE\nTo understand how this works and the ways it differs from previous systems, it is important to know how an autoencoder works, specifically a Maximum Mean Discrepancy Variational Autoencoder.\n[\nAn Autoencoder learns to recreate its training data by compressing the data into a compressed representation called a “latent code” and decompressing it back into the original data. Each latent code is just a vector of numbers with each number constrained within some bounds (between -1, 1 in this case by a Tanh function). You can think of each latent vector as a position on a latent map of input data. In each direction on the map our input data changes in semantic ways.\nThe problem with an Autoencoder is that our latent map has holes. These holes are where latent codes have no set meaning and so result in garbage output from the decoder. To resolve this we use a Variational Autoencoder (VAE). It has a regularising loss function which encourages a smooth distribution of latent codes. This regularising loss encourages our latent codes to match a target probability distribution, usually a bell curve. Now intermediate points on our map are also valid points meaning we can traverse it smoothly.\nUnfortunately, when applying this model to sequences it doesn’t work. The issue comes from our extra loss function and how we decode sequences.\nOur regularising loss encourages the latent space to be smoother but if the loss is brought to near zero the space becomes meaningless. This is tempered by the reconstruction loss which encourages the latent space to be informative. If the decoder is generating a sequence, it has access to the previous tokens in its current sequence which during training are always correct. When the decoder has the option of a slightly random latent code or a nonrandom previous output it ignores the latent code & just looks at its previous tokens.\nThis is known as “posterior collapse” since the posterior is the probability of an event (the next token) given relevant information (the latent code). The resulting model ignores the latent code and so just models a prior distribution of tokens.\nTo solve this the Maximum Mean Discrepancy Variational Autoencoder was made. It is similar to a VAE but instead of the reconstruction loss, it uses an MMD (mean-maximum-discrepancy) loss. The MMD loss measures the similarity between latent codes, between samples from the target distribution and between both latent codes & samples. It optimises the similarity between latent codes & target samples separately to match the similarity between mixed samples.\nThis loss is much softer on the latest codes and solved posterior collapse for my use case. If your keen to learn more about MMD-VAE you should check out this post."
  },
  {
    "objectID": "posts/2020-08-13-Transformers-as-Variational-Autoencoders.html#a-transformer-as-an-mmd-vae",
    "href": "posts/2020-08-13-Transformers-as-Variational-Autoencoders.html#a-transformer-as-an-mmd-vae",
    "title": "Transformers-as-Variational-Autoencoders",
    "section": "A Transformer as an MMD-VAE",
    "text": "A Transformer as an MMD-VAE\nLets put this model to use to generate some structured sequences. The T5 model provided by Huggingface will create the Encoder & Decoder for the sequences. To get a compressed encoding of the inputs, the inputs are first padded to ensure each the sequence is 12 tokens long. Finally, some fully-connected layers compress and then decompress the fixed length encodings. I’ve named this model “T5-VAE”.\n[\nThis model is then trained to recreate its input tokens with the MMD loss on its latent code. Once training is complete it is possible to start generating some sequences!\nI tried out recreating single lines of code from my dataset of 9 million Python state changes. This code comes from real coding solutions so the model will learn more useful snippets of code than if the data was random. However, this also means the code snippets could be more varied.\nHere I step through the latent space between 2 sample code snippets.\n# Intermediate Samples\n0.0 x = a - 1; # Starting latent space\n0.1 x = a - 1;\n0.2 x = a - 1;\n0.3 x = a - 1;\n0.4 x = a + 1;\n0.5 x = a + 2;\n0.6 x = a + 2;\n0.7 x = a + 2 * 2;\n0.8 x = a + 10 * 2;\n0.9 x = a + 10 * 2;\n1.0 x = a + 10 * 2; # Ending latent space\nHere I randomly generate latent codes to see how common syntax errors are.\n# Randomly Sampled Sequences\ner = int(h[3] * 0);\nl.append([False[j] * d); # Invalid Code\ny = '[0 '] = 1; # Invalid Code\nx = int(h[-1] * 0);\nl.append( = 0 + str(x[0 / 1]); # Invalid Code\nx.append(a[da] * 0);\nx =''[0 - 1:0];\nx.append(x.pop(  + 1) ** 0);\nf = int(h[i].pop() + 1);\nx = int(h[-1 - 1]);\nThough the intermediate values are good, just randomly sampling from the latent space occasionally produces invalid outputs."
  },
  {
    "objectID": "posts/2020-08-13-Transformers-as-Variational-Autoencoders.html#use-cases",
    "href": "posts/2020-08-13-Transformers-as-Variational-Autoencoders.html#use-cases",
    "title": "Transformers-as-Variational-Autoencoders",
    "section": "Use cases",
    "text": "Use cases\nNow that we can learn smooth latent spaces of sequences a lot is possible:\n\nLearn a position in latent space\n\nTrain another model take T5-VAE encodings (e.g. representing a tweet) and predict some property (e.g. the number of likes). Now you can get a loss based on your target number of likes and backpropagate that loss to change the latent position of a given tweet. The result should be a tweet optimizer! I’ve got a demo of this coming soon.\n\nDiscover semantic changes in latent space\n\nChange a sequence in one way (e.g. change the tone) and find the difference in latent space. You may be able to apply that change in latent space to other sequences to get a tone changer."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Fras Green",
    "section": "",
    "text": "Pyxel Playground: A Web-Based Game Editor\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nI made a BABA IS YOU demake!\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMaking a Transformer-VAE with JAX.\n\n\n\nML\n\nTransformer-VAE\n\n\n\nAnd how you can make new Transformers with JAX.\n\n\n\n\n\nJun 13, 2021\n\n\n\n\n\n\n\n\n\n\n\n\nInterpolating the internet\n\n\n\nML\n\n\n\nWhat happens when every plausible idea is an inference away.\n\n\n\n\n\nMar 11, 2021\n\n\n\n\n\n\n\n\n\n\n\n\nAn-Improved-Transformer-VAE\n\n\n\nML\n\nLarge prior-free models\n\nTransformer-VAE\n\n\n\nAn easy to use repo with SOTA performance.\n\n\n\n\n\nFeb 23, 2021\n\n\n\n\n\n\n\n\n\n\n\n\nTransformer-VAE-for-Program-Synthesis\n\n\n\nML\n\nLarge prior-free models\n\ncode\n\nTransformer-VAE\n\n\n\nFind the right program in a latent space.\n\n\n\n\n\nAug 25, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nTransformers-as-Variational-Autoencoders\n\n\n\nML\n\nLarge prior-free models\n\nTransformer-VAE\n\n\n\nAvoid posterior collapse even with a huge model.\n\n\n\n\n\nAug 13, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nAn-Avatar-game-with-realistic-physics\n\n\n\ngames\n\nsimulation\n\n\n\nFinding the physics of water bending.\n\n\n\n\n\nJul 8, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nA dataset of ran code\n\n\n\nML\n\ndatasets\n\ncode\n\n\n\nLearning from state changes in 1 million Python programs.\n\n\n\n\n\nJun 25, 2020\n\n\n\n\n\nNo matching items"
  }
]